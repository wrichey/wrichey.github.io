<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Winona Richey</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="ComputerVision.html">Computer Vision</a></li>						
						<li><a href="Modeling.html">Modeling</a></li>
						<li><a href="Guidance.html">Image Gudiance Platform</a></li>
						<li><a href="MachineLearning.html" class="active">Machine Learning</a></li>
					</ul>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Lung Nodule Classification</h1>

							<span class="image fit"><img src="images/BoF.jpg" alt="" /></span>
							<p> At the University of Central Florida's <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision</a>, under the direction of <a href="http://www.cs.ucf.edu/~bagci/">Dr. Ulas Bagci</a>, I implemented hand crafted features for lung nodule detection and classfication. Hand crafted features were obtained from modified methods of <a href="https://ieeexplore.ieee.org/iel7/42/4359023/06960901.pdf?casa_token=cCLcRuyY2tEAAAAA:CJsaDCOVQ7w0bMU6eUMd0AW-FRLbknPWIJwOXnBAL9P_Csz9_NUtH6Tqj40A9av1kCo5BMo">bag of frequencies</a>, and <a href="https://link.springer.com/content/pdf/10.1007/s11265-016-1134-5.pdf">taxonomic indices</a>. We included a robust radius estimation algorithm that resulted in an average error of 1.29 pixels. </p>
							<p> We compared hand-crafted features with features extracted from a pre-trained deep convolutional neural network (CNN) on lung cancer diagnosis. Our results suggest that specifically designed hand-crafted features can have comparable performance to off-the-shelf deep features. Furthermore, by doing a majority voting we showed that these two groups of features act complementary. </br></br>

							<a href="https://www.crcv.ucf.edu/REU/2016/Richey/Winona-Final%20Paper.pdf" class ="button">REU Paper</a> 
							<a href="https://www.crcv.ucf.edu/REU/2016/Richey/Final%20REU%20Poster%20Winona.pptx" class="button">REU Poster</a> 
							</p>

							<p>This work was presented in: </br> 

							<ul> <li> N. Khosravan, <b>W. L. Richey</b>, U. Bagci. “How Deep Can Hand Crafted Features Be?" In IEEE International Engineering in Medicine and Biology Conference (EMBC), 2018. </li></ul> 

							<a href="https://www.researchgate.net/profile/Naji-Khosravan/publication/327351039_How_Deep_Can_Hand-Crafted_Features_Be/links/5b89b652a6fdcc5f8b75bb8d/How-Deep-Can-Hand-Crafted-Features-Be.pdf" class ="button"> Read more from IEEE EBMC 2018</a>
						</p>
						</div>
						<div class="inner">
							<h1 class="major">Automated Cell Transfer Validation</h1>
							<p> For my undergraduate thesis at the Tulane University, under the direction of Dr. Doug Chrissey, I worked on an automatic classification of cell transfers for 3D bioprinting. Real time cell transfer validation through print ribbon image analysis would allow users to tune laser parameters time-efficiently, and reprint voxels that failed on the first attempt. Because the print ribbon dries out in approximately 30 minutes, manual examination through visual or microscope analysis takes valuable time away from printing and traversing the print ribbon to determine regions of interest. Current methods of cell transfer  validation cost valuable time and decrease accuracy of printed constructs. </p>
							<p>Eight print ribbon images were taken before and after cell voxel transfers. Data augmentation of cropping, rotation, mirroring and normalization resulted in 1344 samples with 67% negative and 33% positive samples.Features were extracted from print ribbon images using a pre-trained neural network GoogLeNet, inception v-3, and these features were classified using a support vector machine (SVM). Our radial kernel SVM classifier achieved 100% sensitivity, 71.5% specificity. Our second degree polynomial kernel SVM had an overall accuracy of 83%. The accuracy of this preliminary work shows feasiblity for a  network trained on more images. The method could be expanded and implemented in a closed feedback loop to achieve consistent and reliable cell transfer, eliminating user guesswork and saving time.
							</p>
							<p>This work was included in a publication by S.C Sklare: 
								<ul> <li> S.C. Sklare, <b>W. L. Richey</b>, B.T. Vinson, D.B. Chrisey. “Directed self-assembly software for single cell deposition."  International Journal of Bioprinting, 3(2), 2017.</li></ul> 

							<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575622/" class ="button"> Read more from the International Journal of Bioprinting 2017 </a>
						</div>

					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li><li><a href="index.html#contact">Contact Winona Richey</a> for more info!</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>